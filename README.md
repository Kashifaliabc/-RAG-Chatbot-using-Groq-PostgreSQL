# ğŸ§  RAG Chatbot using Groq, PostgreSQL, and LangChain

A **Retrieval-Augmented Generation (RAG)** chatbot built with **Groq LLMs**, **PostgreSQL with pgvector**, and **LangChain**. This Streamlit-based app allows users to upload PDF documents, store them as vector embeddings, and ask questions with real-time context-aware responses.

---

## ğŸš€ Features

- ğŸ“„ **PDF Upload** â€“ Upload and process any PDF document.
- ğŸ§© **Text Chunking & Embedding** â€“ Uses LangChain's splitter and HuggingFace's sentence-transformers.
- ğŸ—ƒï¸ **Vector Store** â€“ Stores embeddings in a PostgreSQL database using `pgvector`.
- ğŸ¤– **Conversational QA** â€“ Ask questions and get intelligent answers based on the document.
- ğŸ’¬ **Chat Memory** â€“ Remembers previous queries to maintain a contextual conversation.

---

## ğŸ” How It Works

1. ### ğŸ“¤ File Upload  
   Users upload a PDF file which is saved locally for processing.

2. ### âœ‚ï¸ Text Chunking  
   The document is split into smaller text chunks using `RecursiveCharacterTextSplitter` to make it suitable for embedding.

3. ### ğŸ§  Embedding  
   Each text chunk is converted into a dense vector using the HuggingFace model `sentence-transformers/all-MiniLM-L6-v2`.

4. ### ğŸ—‚ï¸ Vector Storage  
   The embeddings are stored in a PostgreSQL database using the `pgvector` extension, allowing for fast similarity search.

5. ### ğŸ’¬ Conversational Querying  
   Users type questions, and the chatbot retrieves relevant chunks and generates answers using the Groq LLM (`llama3-8b-8192`) with chat memory.

---

## âš™ï¸ Setup Guide

# Create virtual environment
python -m venv .venv

# Activate it
# On Windows:
.venv\Scripts\activate

# On macOS/Linux:
source .venv/bin/activate

# Install Dependancy
pip install -r requirements.txt
